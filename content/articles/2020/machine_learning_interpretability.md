+++
title = "Essential Ideas from Machine Learning Interpretability"
summary = "problems and methods for dealing with anchoring bias"
date = 2020-06-08
template = "article.html"
+++

In today's world every business is trying to optimize its production processes. We are all striving for more efficiency, lower costs,
higher margins, better understanding and insight into our markets and improved capabilities of dealing with overall market dynamics. This is nothing new and has been the goal of businessmen even long before Italian trade dynamics blossomed into the renaissance. The current COVID-19 crisis, however, has made everyone - not just entrepreneuers - acutely aware of the necessity of being on the cutting edge of methodologies for forecasting, understanding and adapting to structural changes in the market.

One method for automating, forecasting and augementing our understanding these days is through machine learning models. If we want to know, for example, which customers are likely to leave next quarter or within the next year, practictioners like myself will build models based off of data we have collected which can then be used to make predictions for potential outcomes.

Often our goal as ML practictioners is to have the models we develop be as accurate as possible. For example, given two models *A* and *B*, if model *A* has an accuracy of \\(99.9\\%\\) and *B* has an accuracy of \\(99.5\\%\\) then *A* is preferred over *B*. In many cases this is fine, but due to the ever increasing complexity of our models as well as the fact that regulatory bodies are increasingly asking questions and requiring that our models have interpretable results, we should also be focusing not just on accuracy but also on what those predictions mean. 